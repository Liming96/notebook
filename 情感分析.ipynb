{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 利用 keras 进行点评评论情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、准备工作\n",
    "### 1、数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewid</th>\n",
       "      <th>reviewbody</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>661913194</td>\n",
       "      <td>宝燕乐园的滑滑梯很出名啊，波浪行的，陡峭型的，管道式的，小朋友一个滑滑梯滑过来，乐此不疲。园...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>661913413</td>\n",
       "      <td>挺不错的潮汕味道，老家的味道</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661913671</td>\n",
       "      <td>心心念念几个月，今天终于成行啦，四个人早早开车前往田林路，最近在修路所以车只能停在地下室，（...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>661914092</td>\n",
       "      <td>干净卫生，老板服务热情，很温馨的汗蒸房??</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>661916265</td>\n",
       "      <td>慕名而去的，88元12个的大生蚝吃完感觉特别值，出菜员还告诉我们给我们多加了两个。感觉在广东...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    reviewid                                         reviewbody  star\n",
       "0  661913194  宝燕乐园的滑滑梯很出名啊，波浪行的，陡峭型的，管道式的，小朋友一个滑滑梯滑过来，乐此不疲。园...    50\n",
       "1  661913413                                     挺不错的潮汕味道，老家的味道    50\n",
       "2  661913671  心心念念几个月，今天终于成行啦，四个人早早开车前往田林路，最近在修路所以车只能停在地下室，（...    50\n",
       "3  661914092                              干净卫生，老板服务热情，很温馨的汗蒸房??    50\n",
       "4  661916265  慕名而去的，88元12个的大生蚝吃完感觉特别值，出菜员还告诉我们给我们多加了两个。感觉在广东...    45"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/liming/Downloads/review.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、情感划分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的目的是分析文本的情感：积极或消极。\n",
    "因此，这里设置阈值为30：star 小于30的为消极（0）、大于等于30的为积极（1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewid</th>\n",
       "      <th>reviewbody</th>\n",
       "      <th>star</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>661913194</td>\n",
       "      <td>宝燕乐园的滑滑梯很出名啊，波浪行的，陡峭型的，管道式的，小朋友一个滑滑梯滑过来，乐此不疲。园...</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>661913413</td>\n",
       "      <td>挺不错的潮汕味道，老家的味道</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661913671</td>\n",
       "      <td>心心念念几个月，今天终于成行啦，四个人早早开车前往田林路，最近在修路所以车只能停在地下室，（...</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>661914092</td>\n",
       "      <td>干净卫生，老板服务热情，很温馨的汗蒸房??</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>661916265</td>\n",
       "      <td>慕名而去的，88元12个的大生蚝吃完感觉特别值，出菜员还告诉我们给我们多加了两个。感觉在广东...</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    reviewid                                         reviewbody  star  \\\n",
       "0  661913194  宝燕乐园的滑滑梯很出名啊，波浪行的，陡峭型的，管道式的，小朋友一个滑滑梯滑过来，乐此不疲。园...    50   \n",
       "1  661913413                                     挺不错的潮汕味道，老家的味道    50   \n",
       "2  661913671  心心念念几个月，今天终于成行啦，四个人早早开车前往田林路，最近在修路所以车只能停在地下室，（...    50   \n",
       "3  661914092                              干净卫生，老板服务热情，很温馨的汗蒸房??    50   \n",
       "4  661916265  慕名而去的，88元12个的大生蚝吃完感觉特别值，出菜员还告诉我们给我们多加了两个。感觉在广东...    45   \n",
       "\n",
       "   sentiment  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义函数：根据用户评的星级来估计sentiment（情感）\n",
    "def make_label(star):\n",
    "    if star >=30:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "# 运用 apply 方法得到新列\n",
    "data[\"sentiment\"] = data.star.apply(make_label)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = data[\"reviewbody\"].astype(str)\n",
    "labels = data[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3、分词并转化为词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/zd/qhg48cw17_ncqf0rl48wz5rh0000gp/T/jieba.cache\n",
      "Loading model cost 0.662 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练词向量\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "def train_word2vec(sentences,save_path):\n",
    "    sentences_seg = []\n",
    "    sen_str = \"\\n\".join(sentences)\n",
    "    res = jieba.lcut(sen_str)\n",
    "    seg_str = \" \".join(res)\n",
    "    sen_list = seg_str.split(\"\\n\")\n",
    "    for i in sen_list:\n",
    "        sentences_seg.append(i.split())\n",
    "    print(\"开始训练词向量\") \n",
    "#     logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    model = Word2Vec(sentences_seg,\n",
    "                size=100,  # 词向量维度\n",
    "                min_count=5,  # 词频阈值\n",
    "                window=5)  # 窗口大小    \n",
    "    model.save(save_path)\n",
    "    return model\n",
    "\n",
    "model =  train_word2vec(sentences,'/Users/liming/Downloads/word2vec.model')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "\n",
    "def generate_id2wec(word2vec_model):\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(model.wv.vocab.keys(), allow_update=True)\n",
    "    w2id = {v: k + 1 for k, v in gensim_dict.items()}  # 词语的索引，从1开始编号\n",
    "    w2vec = {word: model[word] for word in w2id.keys()}  # 词语的词向量\n",
    "    n_vocabs = len(w2id) + 1\n",
    "    embedding_weights = np.zeros((n_vocabs, 100))\n",
    "    for w, index in w2id.items():  # 从索引为1的词语开始，用词向量填充矩阵\n",
    "        embedding_weights[index, :] = w2vec[w]\n",
    "    return w2id,embedding_weights\n",
    "\n",
    "def text_to_array(w2index, senlist):  # 文本转为索引数字模式\n",
    "    sentences_array = []\n",
    "    for sen in senlist:\n",
    "        new_sen = [ w2index.get(word,0) for word in sen]   # 单词转索引数字\n",
    "        sentences_array.append(new_sen)\n",
    "    return np.array(sentences_array)\n",
    "\n",
    "def prepare_data(w2id,sentences,labels,max_len=200):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(sentences,labels, test_size=0.2)\n",
    "    X_train = text_to_array(w2id, X_train)\n",
    "    X_val = text_to_array(w2id, X_val)\n",
    "    X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "    X_val = pad_sequences(X_val, maxlen=max_len)\n",
    "    return np.array(X_train), np_utils.to_categorical(y_train) ,np.array(X_val), np_utils.to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "w2id, embedding_weights = generate_id2wec(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import np_utils\n",
    "\n",
    "x_train, y_trian, x_val , y_val = prepare_data(w2id, sentences, labels,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment:\n",
    "    def __init__(self,w2id,embedding_weights,Embedding_dim,maxlen,labels_category):\n",
    "        self.Embedding_dim = Embedding_dim\n",
    "        self.embedding_weights = embedding_weights\n",
    "        self.vocab = w2id\n",
    "        self.labels_category = labels_category\n",
    "        self.maxlen = maxlen\n",
    "        self.model = self.build_model()\n",
    "      \n",
    "        \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        #input dim(140,100)\n",
    "        model.add(Embedding(output_dim = self.Embedding_dim,\n",
    "                           input_dim=len(self.vocab)+1,\n",
    "                           weights=[self.embedding_weights],\n",
    "                           input_length=self.maxlen))\n",
    "        model.add(Bidirectional(LSTM(50),merge_mode='concat'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(self.labels_category))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer='adam', \n",
    "                     metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def train(self,X_train, y_train,X_test, y_test,n_epoch=5 ):\n",
    "        self.model.fit(X_train, y_train, batch_size=32, epochs=n_epoch,\n",
    "                      validation_data=(X_test, y_test))\n",
    "        self.model.save('sentiment.h5')   \n",
    "        \n",
    "    def predict(self,model_path,new_sen):\n",
    "        model = self.model\n",
    "        model.load_weights(model_path)\n",
    "        new_sen_list = jieba.lcut(new_sen)\n",
    "        sen2id =[ self.vocab.get(word,0) for word in new_sen_list]\n",
    "        sen_input = pad_sequences([sen2id], maxlen=self.maxlen)\n",
    "        res = model.predict(sen_input)[0]\n",
    "        return np.argmax(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 200, 100)          2885200   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100)               60400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 202       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,945,802\n",
      "Trainable params: 2,945,802\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Bidirectional,LSTM,Dense,Embedding,Dropout,Activation,Softmax\n",
    "\n",
    "senti = Sentiment(w2id,embedding_weights,100,200,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/1\n",
      "80000/80000 [==============================] - 1776s 22ms/step - loss: 0.1473 - accuracy: 0.9518 - val_loss: 0.1270 - val_accuracy: 0.9569\n"
     ]
    }
   ],
   "source": [
    "senti.train(x_train,y_trian, x_val ,y_val,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'这家的银耳莲子羹很不错，上菜很快，菜的照片很真实'的情感是:\n",
      "积极的\n"
     ]
    }
   ],
   "source": [
    "label_dic = {0:\"消极的\",1:\"积极的\"}\n",
    "sen_new = \"这家的银耳莲子羹很不错，上菜很快，菜的照片很真实\"\n",
    "pre = senti.predict(\"./sentiment.h5\",sen_new)\n",
    "print(\"'{}'的情感是:\\n{}\".format(sen_new,label_dic.get(pre)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
