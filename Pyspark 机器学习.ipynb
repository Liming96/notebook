{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、回归（Regression）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('cruise_ship_info.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|  Ship_name|Cruise_line|Age|           Tonnage|passengers|length|cabins|passenger_density|crew|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|    Journey|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
      "|      Quest|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
      "|Celebration|   Carnival| 26|            47.262|     14.86|  7.22|  7.43|             31.8| 6.7|\n",
      "|   Conquest|   Carnival| 11|             110.0|     29.74|  9.53| 14.88|            36.99|19.1|\n",
      "|    Destiny|   Carnival| 17|           101.353|     26.42|  8.92| 13.21|            38.36|10.0|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）将标签数据转化为整数索引\n",
    "因为要运用回归模型，所以需要将标签字段（分类字段）转换为数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## StringIndexer是一个Estimator，用来将某个文本属性的值转化成数字编码index，以便后续其他适用于数字编码的算法使用。\n",
    "## 编码规则是对该文本属性每个出现的属性值label给出从0～label数量-1的数字，出现频率越高的label，给出的编码数字就越小。\n",
    "## 因此StringIndexer是需要根据已有训练集来进行fit的。\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+----------+\n",
      "|  Ship_name|Cruise_line|Age|           Tonnage|passengers|length|cabins|passenger_density|crew|cruise_cat|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+----------+\n",
      "|    Journey|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|      16.0|\n",
      "|      Quest|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|      16.0|\n",
      "|Celebration|   Carnival| 26|            47.262|     14.86|  7.22|  7.43|             31.8| 6.7|       1.0|\n",
      "|   Conquest|   Carnival| 11|             110.0|     29.74|  9.53| 14.88|            36.99|19.1|       1.0|\n",
      "|    Destiny|   Carnival| 17|           101.353|     26.42|  8.92| 13.21|            38.36|10.0|       1.0|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"Cruise_line\", outputCol=\"cruise_cat\")\n",
    "## 根据已有训练集进行 fit\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）将多列特征组合成一个向量列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VectorAssembler是一个转换器,它可以将给定的多列转换为一个向量列\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['Age',\n",
    "               'Tonnage',\n",
    "               'passengers',\n",
    "               'length',\n",
    "               'cabins',\n",
    "               'passenger_density',\n",
    "               'cruise_cat'],\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|            features|crew|\n",
      "+--------------------+----+\n",
      "|[6.0,30.276999999...|3.55|\n",
      "|[6.0,30.276999999...|3.55|\n",
      "|[26.0,47.262,14.8...| 6.7|\n",
      "|[11.0,110.0,29.74...|19.1|\n",
      "|[17.0,101.353,26....|10.0|\n",
      "+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = assembler.transform(indexed)\n",
    "output.select(\"features\", \"crew\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）将数据集划分为训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = output.select(\"features\", \"crew\")\n",
    "train_data,test_data = full_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （4）训练线性回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建线性回归模型并训练\n",
    "lr = LinearRegression(featuresCol = 'features',labelCol='crew',predictionCol='prediction')\n",
    "lrModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01415926727704148,0.006120844210220613,-0.15060788148792473,0.4560453232842637,0.8690266483207997,-0.0006548166180796964,0.04433218409250203]\n",
      "-1.1598221050189703\n"
     ]
    }
   ],
   "source": [
    "## 模型系数和截距\n",
    "print(lrModel.coefficients)\n",
    "print(lrModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0132983612553066\n",
      "0.908675109913899\n"
     ]
    }
   ],
   "source": [
    "## 模型训练均方根差（RMSE）和 R方\n",
    "trainingSummary = lrModel.summary\n",
    "print(trainingSummary.rootMeanSquaredError)\n",
    "print(trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "| -1.3832551030447302|\n",
      "|  0.5516827126047827|\n",
      "|0.007265278652305085|\n",
      "| -0.8206717806779125|\n",
      "| -0.8206717806779125|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 模型训练残差\n",
    "trainingSummary.residuals.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （5）评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = lrModel.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6355876682111081\n",
      "0.40397168398203365\n",
      "0.974274797868934\n"
     ]
    }
   ],
   "source": [
    "## 模型测试均方根差（RMSE）、均方误差（MSE）和 R方\n",
    "print(test_results.rootMeanSquaredError)\n",
    "print(test_results.meanSquaredError)\n",
    "print(test_results.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------------------+\n",
      "|            features|crew|        prediction|\n",
      "+--------------------+----+------------------+\n",
      "|[4.0,220.0,54.0,1...|21.0| 20.82479894863448|\n",
      "|[5.0,115.0,35.74,...|12.2|11.886366778288501|\n",
      "|[5.0,160.0,36.34,...|13.6|15.108232838175828|\n",
      "|[6.0,113.0,37.82,...|12.0| 11.68772199562538|\n",
      "|[9.0,90.09,25.01,...|8.69| 9.368189276414368|\n",
      "+--------------------+----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 输出测试数据的真实值和预测值\n",
    "test_results.predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （6）模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|        prediction|\n",
      "+--------------------+------------------+\n",
      "|[4.0,220.0,54.0,1...| 20.82479894863448|\n",
      "|[5.0,115.0,35.74,...|11.886366778288501|\n",
      "|[5.0,160.0,36.34,...|15.108232838175828|\n",
      "|[6.0,113.0,37.82,...| 11.68772199562538|\n",
      "|[9.0,90.09,25.01,...| 9.368189276414368|\n",
      "+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = lrModel.transform(test_data.select('features'))\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补充：计算模型某些特征与标签的相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|corr(crew, passengers)|\n",
      "+----------------------+\n",
      "|    0.9152341306065384|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(corr('crew','passengers')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|corr(crew, cabins)|\n",
      "+------------------+\n",
      "|0.9508226063578497|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(corr('crew','cabins')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、分类（Classification）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('customer_churn.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+--------------------+-----+\n",
      "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|             Company|Churn|\n",
      "+----------------+----+--------------+---------------+-----+---------+--------------------+-----+\n",
      "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|          Harvey LLC|    1|\n",
      "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|          Wilson PLC|    1|\n",
      "|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|Miller, Johnson a...|    1|\n",
      "|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|           Smith Inc|    1|\n",
      "|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|          Love-Jones|    1|\n",
      "+----------------+----+--------------+---------------+-----+---------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Onboard_date 和 Location 在这里不会用到，为了方便观察，这里不显示\n",
    "data.drop('Onboard_date','Location').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+--------------------+-----+\n",
      "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|             Company|Churn|\n",
      "+----------------+----+--------------+---------------+-----+---------+--------------------+-----+\n",
      "|    Kayla Reeves|38.0|         100.0|              0| 5.27|      5.0|       Stewart-Lopez|    0|\n",
      "|   Justin Campos|53.0|        3263.0|              1| 2.77|      9.0|         Hall-Butler|    0|\n",
      "|     Lori Medina|39.0|       3676.68|              1| 3.52|      9.0|Garcia, Hansen an...|    0|\n",
      "|     Kelly Terry|45.0|       3689.95|              1| 5.01|     11.0|Ellis, Johnston a...|    0|\n",
      "|Kathleen Marquez|35.0|        3825.7|              0| 4.28|      8.0|Steele, Nguyen an...|    0|\n",
      "+----------------+----+--------------+---------------+-----+---------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.drop('Onboard_date','Location').orderBy('Total_Purchase').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips：将连续特征转换为分类特征\n",
    "这里是一个分类问题：预测员工是否会离职。因此需要将连续字段转换为分类字段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Binarizer, Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|   Total_Purchase|\n",
      "+-------+-----------------+\n",
      "|  count|              900|\n",
      "|   mean|10062.82403333334|\n",
      "| stddev|2408.644531858096|\n",
      "|    min|            100.0|\n",
      "|    max|         18026.01|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 将数值特征转化为二值特征，threshold 参数表示决定二值化的阈值\n",
    "## 为了设置 threshold 参数的大小，首先需要对 Total_Purchase 字段进行探索性统计分析\n",
    "data.describe(\"Total_Purchase\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----------------+-----------------+------------------+-------------------+\n",
      "|summary|              Age|   Total_Purchase|            Years|         Num_Sites|              Churn|\n",
      "+-------+-----------------+-----------------+-----------------+------------------+-------------------+\n",
      "|  count|              900|              900|              900|               900|                900|\n",
      "|   mean|41.81666666666667|10062.82403333334| 5.27315555555555| 8.587777777777777|0.16666666666666666|\n",
      "| stddev|6.127560416916251|2408.644531858096|1.274449013194616|1.7648355920350969| 0.3728852122772358|\n",
      "|    min|             22.0|            100.0|              1.0|               3.0|                  0|\n",
      "|    25%|             38.0|          8480.93|             4.45|               7.0|                  0|\n",
      "|    50%|             42.0|         10041.13|             5.21|               8.0|                  0|\n",
      "|    75%|             46.0|         11758.69|             6.11|              10.0|                  0|\n",
      "|    max|             65.0|         18026.01|             9.15|              14.0|                  1|\n",
      "+-------+-----------------+-----------------+-----------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.drop('Names','Onboard_date','Location','Company','Account_Manager').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+--------------+---------------+-----+---------+-----+\n",
      "|             Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|Churn|\n",
      "+------------------+----+--------------+---------------+-----+---------+-----+\n",
      "|     Ethan Cordova|39.0|      18026.01|              1| 3.82|      9.0|    0|\n",
      "|      Kevin Powell|43.0|      16955.76|              0| 7.04|      8.0|    0|\n",
      "|        Eric Terry|42.0|      16371.42|              1| 3.84|     10.0|    0|\n",
      "|      Holly Flores|47.0|      15878.11|              1| 2.05|      8.0|    0|\n",
      "|   Darin Alexander|43.0|      15858.91|              1| 4.51|      8.0|    0|\n",
      "|  Michael Williams|35.0|      15571.26|              0| 6.45|      9.0|    0|\n",
      "|     Kenneth James|41.0|      15516.52|              0| 3.53|     10.0|    0|\n",
      "|Catherine Johnston|38.0|      15509.97|              0| 4.65|      8.0|    0|\n",
      "|      Katie Wagner|43.0|      15423.03|              1| 2.41|      7.0|    0|\n",
      "|    Brandon Hunter|45.0|      15188.65|              0| 6.17|      8.0|    0|\n",
      "|       Erin Norris|37.0|      15070.32|              0| 6.91|      6.0|    0|\n",
      "|    Phillip Spears|52.0|      14838.84|              0| 5.12|      8.0|    0|\n",
      "|     Jessica Wells|41.0|      14738.09|              1|  6.5|      8.0|    0|\n",
      "|       Wendy Moore|41.0|      14722.35|              0| 6.98|      6.0|    0|\n",
      "|     Sharon Torres|36.0|      14715.53|              1| 5.73|      9.0|    0|\n",
      "|    Jessica Flores|52.0|      14669.61|              0| 6.28|      9.0|    0|\n",
      "|      Keith Bowman|46.0|       14664.0|              0| 6.54|      8.0|    0|\n",
      "|       Manuel Hill|37.0|      14595.51|              1| 3.83|     12.0|    0|\n",
      "|      Heidi Butler|39.0|      14425.74|              0| 5.91|      6.0|    0|\n",
      "|     Lindsey Adams|46.0|      14361.38|              0| 4.52|      8.0|    0|\n",
      "+------------------+----+--------------+---------------+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.drop('Onboard_date','Location','Company').filter(\"Churn==0\").orderBy('Total_Purchase',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+--------------+---------------+-----+---------+-----+\n",
      "|            Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|Churn|\n",
      "+-----------------+----+--------------+---------------+-----+---------+-----+\n",
      "|      Amy Griffin|48.0|       4771.65|              0| 3.77|     12.0|    1|\n",
      "| Brittany Hopkins|55.0|       5024.52|              0| 8.11|      9.0|    1|\n",
      "|       David Hess|41.0|       5192.38|              1| 4.86|     11.0|    1|\n",
      "|   Lindsay Martin|53.0|       5515.09|              0| 6.85|      8.0|    1|\n",
      "|     Mary Aguilar|50.0|       6244.75|              0| 4.64|     11.0|    1|\n",
      "|Mr. Jerome Dawson|36.0|       6330.43|              1| 5.43|      7.0|    1|\n",
      "|      Alexis Hill|39.0|       6351.79|              0| 5.86|      6.0|    1|\n",
      "|  Cheyenne Rogers|36.0|       6447.99|              1| 5.52|     11.0|    1|\n",
      "|       Adam Gomez|48.0|       6495.01|              1| 5.57|     12.0|    1|\n",
      "|   Harold Griffin|41.0|       6569.87|              1|  4.3|     11.0|    1|\n",
      "| Stephen Callahan|42.0|       6635.19|              0| 6.68|     11.0|    1|\n",
      "|      Randy Hayes|43.0|       6715.23|              0| 4.16|      8.0|    1|\n",
      "|  Daniel Bartlett|45.0|       6749.49|              0| 5.88|     14.0|    1|\n",
      "|   Jessica Horton|43.0|       6992.09|              1| 6.84|     11.0|    1|\n",
      "|   Kenneth Bryant|47.0|       7222.35|              0| 6.41|     11.0|    1|\n",
      "|    Russell Bauer|38.0|       7287.57|              1| 7.39|     11.0|    1|\n",
      "|     David Montes|45.0|       7351.38|              0| 5.76|     11.0|    1|\n",
      "| Patrick Robinson|47.0|        7396.1|              0| 4.11|     11.0|    1|\n",
      "| Steven Stevenson|52.0|       7460.05|              0| 5.39|     12.0|    1|\n",
      "|    Raymond Berry|41.0|       7777.37|              0| 4.81|     12.0|    1|\n",
      "+-----------------+----+--------------+---------------+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.drop('Onboard_date','Location','Company').filter(\"Churn==1\").orderBy('Total_Purchase').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到 Total_Purchase 字段的中位数和均值都在10000左右，因此阈值选为10000。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizer = Binarizer(threshold=10000, inputCol='Total_Purchase', outputCol='Total_Purchase_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据阈值列表（分割的参数），将连续变量转换为多项值（连续变量离散化到指定的范围区间）\n",
    "# 提供5个分割点意味着产生4类\n",
    "bucketizer = Bucketizer(splits=[0, 10, 30, 50, 70], inputCol='Age', outputCol='age_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline stages\n",
    "from pyspark.ml import Pipeline\n",
    "stages = [binarizer, bucketizer]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+--------------+---------------+-----+---------+-----+------------------+-------+\n",
      "|           Names| Age|Total_Purchase|Account_Manager|Years|Num_Sites|Churn|Total_Purchase_cat|age_cat|\n",
      "+----------------+----+--------------+---------------+-----+---------+-----+------------------+-------+\n",
      "|Cameron Williams|42.0|       11066.8|              0| 7.22|      8.0|    1|               1.0|    2.0|\n",
      "|   Kevin Mueller|41.0|      11916.22|              0|  6.5|     11.0|    1|               1.0|    2.0|\n",
      "|     Eric Lozano|38.0|      12884.75|              0| 6.67|     12.0|    1|               1.0|    2.0|\n",
      "|   Phillip White|42.0|       8010.76|              0| 6.71|     10.0|    1|               0.0|    2.0|\n",
      "|  Cynthia Norton|37.0|       9191.58|              0| 5.56|      9.0|    1|               0.0|    2.0|\n",
      "+----------------+----+--------------+---------------+-----+---------+-----+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit the pipeline model and transform the data\n",
    "result = pipeline.fit(data).transform(data)\n",
    "result.drop('Onboard_date','Location','Company').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）将多列特征组合成一个向量列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Age',\n",
    "                                       'Total_Purchase',\n",
    "                                       'Account_Manager',\n",
    "                                       'Years',\n",
    "                                       'Num_Sites'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|churn|\n",
      "+--------------------+-----+\n",
      "|[42.0,11066.8,0.0...|    1|\n",
      "|[41.0,11916.22,0....|    1|\n",
      "|[38.0,12884.75,0....|    1|\n",
      "|[42.0,8010.76,0.0...|    1|\n",
      "|[37.0,9191.58,0.0...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = output.select('features','churn')\n",
    "final_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_churn,test_churn = final_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）选择模型并训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法一：逻辑回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_churn = LogisticRegression(featuresCol = 'features',labelCol='churn')\n",
    "model = lr_churn.fit(train_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[22.0,11254.38,1....|  0.0|[4.29752479205010...|[0.98658035094082...|       0.0|\n",
      "|[25.0,9672.03,0.0...|  0.0|[4.45078681080970...|[0.98846522196675...|       0.0|\n",
      "|[26.0,8787.39,1.0...|  1.0|[0.42481734257158...|[0.60463542102943...|       0.0|\n",
      "|[27.0,8628.8,1.0,...|  0.0|[5.16036305292277...|[0.99429313966103...|       0.0|\n",
      "|[28.0,8670.98,0.0...|  0.0|[7.47283595367025...|[0.99943200850135...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_sum = model.summary\n",
    "training_sum.predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （4）模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator,MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[26.0,8939.61,0.0...|    0|[6.09287719449442...|[0.99774619086945...|       0.0|\n",
      "|[28.0,11245.38,0....|    0|[3.64367153236502...|[0.97451057057915...|       0.0|\n",
      "|[29.0,9378.24,0.0...|    0|[4.57063641463622...|[0.98975468320583...|       0.0|\n",
      "|[30.0,6744.87,0.0...|    0|[3.31883963587618...|[0.96506949627380...|       0.0|\n",
      "|[31.0,10058.87,1....|    0|[4.30854259864759...|[0.98672544256430...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 代入测试集\n",
    "pred_and_labels = model.evaluate(test_churn)\n",
    "pred_and_labels.predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.885"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='churn')\n",
    "churn_eval_multi = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='churn',metricName='accuracy')\n",
    "auc = churn_eval_multi.evaluate(pred_and_labels.predictions)\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "效果好像不太好？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法二：决策树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier,DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,[0,1,3,4],[0.09120095289461654,0.08511254111869927,0.1432383156878679,0.6804481902988163])\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(labelCol='churn',featuresCol='features')\n",
    "dtc_model = dtc.fit(train_churn)\n",
    "print(dtc_model.featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = dtc_model.transform(test_churn)\n",
    "accuracy = churn_eval_multi.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个更不好？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法三：随机森林模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,[0,1,2,3,4],[0.10313914486830404,0.08820488003917278,0.022502521370334393,0.13157948168406858,0.6545739720381203])\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(labelCol=\"churn\", featuresCol=\"features\", numTrees=20)\n",
    "rfc_model = rfc.fit(train_churn)\n",
    "print(rfc_model.featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = rfc_model.transform(test_churn)\n",
    "accuracy = churn_eval_multi.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "咋回事？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法四：梯度提升树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(labelCol=\"churn\", featuresCol=\"features\", maxIter=20)\n",
    "gbt_model = gbt.fit(train_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = gbt_model.transform(test_churn)\n",
    "accuracy = churn_eval_multi.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、聚类（Clustering）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"hack_data.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------+---------------+-----------------+---------------+----------------+\n",
      "|Session_Connection_Time|Bytes Transferred|Kali_Trace_Used|Servers_Corrupted|Pages_Corrupted|WPM_Typing_Speed|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+----------------+\n",
      "|                    8.0|           391.09|              1|             2.96|            7.0|           72.37|\n",
      "|                   20.0|           720.99|              0|             3.04|            9.0|           69.08|\n",
      "|                   31.0|           356.32|              1|             3.71|            8.0|           70.58|\n",
      "|                    2.0|           228.08|              1|             2.48|            8.0|            70.8|\n",
      "|                   20.0|            408.5|              0|             3.57|            8.0|           71.28|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.drop('Location').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）将多列特征组合成一个向量列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = ['Session_Connection_Time', 'Bytes Transferred', 'Kali_Trace_Used',\n",
    "             'Servers_Corrupted', 'Pages_Corrupted','WPM_Typing_Speed']\n",
    "vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol='features')\n",
    "final_data = vec_assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[8.0,391.09,1.0,2...|\n",
      "|[20.0,720.99,0.0,...|\n",
      "|[31.0,356.32,1.0,...|\n",
      "|[2.0,228.08,1.0,2...|\n",
      "|[20.0,408.5,0.0,3...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.select('features').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）特征标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "cluster_final_data = scaler.fit(final_data).transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      scaledFeatures|\n",
      "+--------------------+\n",
      "|[0.56785108466505...|\n",
      "|[1.41962771166263...|\n",
      "|[2.20042295307707...|\n",
      "|[0.14196277116626...|\n",
      "|[1.41962771166263...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_final_data.select(\"scaledFeatures\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）K-Means 聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(featuresCol='scaledFeatures',k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434.75507308487647"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.fit(cluster_final_data)\n",
    "model.computeCost(cluster_final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.26023837, 1.31829808, 0.99280765, 1.36491885, 2.5625043 ,\n",
       "        5.26676612]),\n",
       " array([3.05623261, 2.95754486, 1.99757683, 3.2079628 , 4.49941976,\n",
       "        3.26738378]),\n",
       " array([2.93719177, 2.88492202, 0.        , 3.19938371, 4.52857793,\n",
       "        3.30407351])]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.clusterCenters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （4）模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|   88|\n",
      "|         2|   79|\n",
      "|         0|  167|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(cluster_final_data).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+--------------------+--------------------+----------+\n",
      "|Session_Connection_Time|Bytes Transferred|Kali_Trace_Used|Servers_Corrupted|Pages_Corrupted|            Location|WPM_Typing_Speed|            features|      scaledFeatures|prediction|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+--------------------+--------------------+----------+\n",
      "|                    8.0|           391.09|              1|             2.96|            7.0|            Slovenia|           72.37|[8.0,391.09,1.0,2...|[0.56785108466505...|         0|\n",
      "|                   20.0|           720.99|              0|             3.04|            9.0|British Virgin Is...|           69.08|[20.0,720.99,0.0,...|[1.41962771166263...|         0|\n",
      "|                   31.0|           356.32|              1|             3.71|            8.0|             Tokelau|           70.58|[31.0,356.32,1.0,...|[2.20042295307707...|         0|\n",
      "|                    2.0|           228.08|              1|             2.48|            8.0|             Bolivia|            70.8|[2.0,228.08,1.0,2...|[0.14196277116626...|         0|\n",
      "|                   20.0|            408.5|              0|             3.57|            8.0|                Iraq|           71.28|[20.0,408.5,0.0,3...|[1.41962771166263...|         0|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(cluster_final_data).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+\n",
      "|            features|      scaledFeatures|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|[8.0,391.09,1.0,2...|[0.56785108466505...|         0|\n",
      "|[20.0,720.99,0.0,...|[1.41962771166263...|         0|\n",
      "|[31.0,356.32,1.0,...|[2.20042295307707...|         0|\n",
      "|[2.0,228.08,1.0,2...|[0.14196277116626...|         0|\n",
      "|[20.0,408.5,0.0,3...|[1.41962771166263...|         0|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(cluster_final_data).select('features','scaledFeatures','prediction').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4、基于TF-IDF 算法的文本挖掘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"SMSSpamCollection\",inferSchema=True,sep='\\t')\n",
    "data = data.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，这是一个邮件及其类别的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute length of each text\n",
    "data = data.withColumn('length',length(data['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover,CountVectorizer,IDF,StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"stop_tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 去除停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算逆文本频率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将类标签由字符串映射到索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）将列转化为模型输入特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （4）划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,count_vec,idf,clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = data_prep_pipe.fit(data)\n",
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = clean_data.select(['label','features'])\n",
    "(train_data,test_data) = full_data.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （5）模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nb.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （6）模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13588,[0,1,2,3,4...|[-3592.6536481156...|[1.0,3.5894638863...|       0.0|\n",
      "|  0.0|(13588,[0,1,2,3,4...|[-2823.9774728892...|[1.0,3.0240329484...|       0.0|\n",
      "|  0.0|(13588,[0,1,2,3,4...|[-3095.2908236727...|[1.0,8.8358217240...|       0.0|\n",
      "|  0.0|(13588,[0,1,2,3,4...|[-1075.7111968609...|[1.0,2.8629849890...|       0.0|\n",
      "|  0.0|(13588,[0,1,2,3,5...|[-1787.4043923033...|[1.0,2.0766777717...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results = model.transform(test_data)\n",
    "test_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting spam was: 0.9410145943960191\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting spam was: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
